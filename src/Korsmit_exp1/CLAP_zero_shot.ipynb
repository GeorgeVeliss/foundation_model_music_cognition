{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e6b3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: change this to the path in your setup\n",
    "korsmit_exp1_path = \"../../data/Korsmit/Exp1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f635ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ClapModel, AutoProcessor\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6586ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLAP model + processor\n",
    "model = ClapModel.from_pretrained(\"laion/larger_clap_music\")\n",
    "processor = AutoProcessor.from_pretrained(\"laion/larger_clap_music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e87de958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of CLAP's parameters: 193913882\n"
     ]
    }
   ],
   "source": [
    "print('total number of CLAP\\'s parameters:', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a16bd87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAP model size: 740.294MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('CLAP model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df0e32",
   "metadata": {},
   "source": [
    "# Process audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2b960e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_stimuli = []\n",
    "stimuli_path = korsmit_exp1_path+\"Stimuli/\"\n",
    "\n",
    "for file in sorted(os.listdir(stimuli_path)):\n",
    "    if file.endswith(\".wav\"):\n",
    "        wav_path = os.path.join(stimuli_path, file)\n",
    "        audio, sample_rate = librosa.load(wav_path, sr=48000)\n",
    "        audio_stimuli.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(audios=audio_stimuli, return_tensors=\"pt\", padding=True, sampling_rate=48000)\n",
    "audio_embeddings = model.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f03a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 512])\n"
     ]
    }
   ],
   "source": [
    "print(audio_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1aa71",
   "metadata": {},
   "source": [
    "# Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f16d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I perceive this sound as happiness', 'I perceive this sound as sadness', 'I perceive this sound as anger', 'I perceive this sound as tenderness', 'I perceive this sound as fear']\n",
      "['This sound makes me feel happiness', 'This sound makes me feel sadness', 'This sound makes me feel anger', 'This sound makes me feel tenderness', 'This sound makes me feel fear']\n",
      "['I perceive this sound as positive', 'I perceive this sound as relaxed', 'I perceive this sound as awake']\n",
      "['This sound makes me feel positive', 'This sound makes me feel relaxed', 'This sound makes me feel awake']\n"
     ]
    }
   ],
   "source": [
    "discrete_tags = [\"happiness\", \"sadness\", \"anger\", \"tenderness\", \"fear\"]\n",
    "\n",
    "discrete_captions_perceived = [\"I perceive this sound as \" + tag for tag in discrete_tags]\n",
    "print(discrete_captions_perceived)\n",
    "discrete_captions_induced = [\"This sound makes me feel \" + tag for tag in discrete_tags]\n",
    "print(discrete_captions_induced)\n",
    "\n",
    "dimensional_tags = [\"positive\", \"relaxed\", \"awake\"]\n",
    "\n",
    "dimensional_captions_perceived = [\"I perceive this sound as \" + tag for tag in dimensional_tags]\n",
    "print(dimensional_captions_perceived)\n",
    "dimensional_captions_induced = [\"This sound makes me feel \" + tag for tag in dimensional_tags]\n",
    "print(dimensional_captions_induced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032319cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = discrete_captions_perceived + discrete_captions_induced + dimensional_captions_perceived + dimensional_captions_induced\n",
    "\n",
    "# NOTE: currently using only dimensional_captions_induced\n",
    "tag_inputs = processor(text=dimensional_captions_induced, return_tensors=\"pt\", padding=True)\n",
    "tag_embeds = model.get_text_features(**tag_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0d14f",
   "metadata": {},
   "source": [
    "# Generate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 3])\n"
     ]
    }
   ],
   "source": [
    "sims = torch.matmul(audio_embeddings, tag_embeds.T)\n",
    "print(sims.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a66c1",
   "metadata": {},
   "source": [
    "## Load csv files and extract related columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mean Vectors (Mean across CSVs for each row position) ---\n",
      "Length of positive_vector: 59\n",
      "Length of relaxed_vector: 59\n",
      "Length of awake_vector: 59\n"
     ]
    }
   ],
   "source": [
    "IDim_path = korsmit_exp1_path+\"Data/IDim/\"\n",
    "IDim_responses = []\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for file in os.listdir(IDim_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(IDim_path, file))\n",
    "        df.columns = df.columns.str.strip()\n",
    "        required_columns = ['positive', 'relaxed', 'awake']\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            all_dfs.append(df)\n",
    "\n",
    "mean_positive_vector = []\n",
    "mean_relaxed_vector = []\n",
    "mean_awake_vector = []\n",
    "\n",
    "num_rows = all_dfs[0].shape[0]\n",
    "\n",
    "for i in range(num_rows):\n",
    "    current_row_positives = []\n",
    "    current_row_relaxeds = []\n",
    "    current_row_awakes = []\n",
    "\n",
    "    # For the current row index 'i', collect values from all DataFrames\n",
    "    for df in all_dfs:\n",
    "        current_row_positives.append(df.iloc[i]['positive'])\n",
    "        current_row_relaxeds.append(df.iloc[i]['relaxed'])\n",
    "        current_row_awakes.append(df.iloc[i]['awake'])\n",
    "\n",
    "    # Calculate the mean for the current row across all files, for each column\n",
    "    mean_positive_vector.append(np.mean(current_row_positives))\n",
    "    mean_relaxed_vector.append(np.mean(current_row_relaxeds))\n",
    "    mean_awake_vector.append(np.mean(current_row_awakes))\n",
    "\n",
    "IDim_responses = {\n",
    "    'positive_vector': mean_positive_vector,\n",
    "    'relaxed_vector': mean_relaxed_vector,\n",
    "    'awake_vector': mean_awake_vector\n",
    "}\n",
    "\n",
    "print(\"\\n--- Mean Vectors (Mean across CSVs for each row position) ---\")\n",
    "print(f\"Length of positive_vector: {len(IDim_responses['positive_vector'])}\")\n",
    "print(f\"Length of relaxed_vector: {len(IDim_responses['relaxed_vector'])}\")\n",
    "print(f\"Length of awake_vector: {len(IDim_responses['awake_vector'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6f9d9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled sims shape: torch.Size([59, 3])\n",
      "Scaled sims (first 5 rows):\n",
      "tensor([[7.1646, 7.0362, 6.7361],\n",
      "        [5.4811, 5.3204, 4.9815],\n",
      "        [3.6491, 3.4734, 3.0879],\n",
      "        [3.1602, 3.1239, 2.7137],\n",
      "        [7.9248, 7.7499, 7.4512],\n",
      "        [8.4446, 8.2235, 7.9176],\n",
      "        [2.8711, 2.7444, 2.4005],\n",
      "        [4.6984, 4.6478, 4.2677],\n",
      "        [6.2722, 6.0907, 5.6706],\n",
      "        [5.0021, 4.9391, 4.5388]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Scaled sims min value: 1.0000\n",
      "Scaled sims max value: 9.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the min and max values in the current sims tensor\n",
    "old_min = sims.min()\n",
    "old_max = sims.max()\n",
    "\n",
    "# Define the new desired range\n",
    "new_min = 1.0\n",
    "new_max = 9.0\n",
    "\n",
    "# Apply the min-max scaling formula\n",
    "scaled_sims = ((sims - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(f\"Scaled sims shape: {scaled_sims.shape}\")\n",
    "print(f\"Scaled sims (first 5 rows):\\n{scaled_sims[:10]}\\n\")\n",
    "print(f\"Scaled sims min value: {scaled_sims.min():.4f}\")\n",
    "print(f\"Scaled sims max value: {scaled_sims.max():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human ratings tensor shape: torch.Size([59, 3])\n",
      "Human ratings tensor (first 5 rows):\n",
      "tensor([[4.4872, 4.8798, 4.5185],\n",
      "        [4.7682, 5.3057, 4.7414],\n",
      "        [5.3105, 5.7312, 5.0760],\n",
      "        [5.2582, 5.2651, 5.4488],\n",
      "        [3.8897, 3.9540, 5.3549],\n",
      "        [4.3197, 3.8900, 5.5089],\n",
      "        [4.9825, 5.1875, 5.5354],\n",
      "        [5.0934, 4.8657, 5.8208],\n",
      "        [4.7605, 4.8502, 5.4158],\n",
      "        [5.2577, 4.8688, 5.7003]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "human_ratings_tensor = torch.tensor([\n",
    "    IDim_responses['positive_vector'],\n",
    "    IDim_responses['relaxed_vector'],\n",
    "    IDim_responses['awake_vector']\n",
    "], dtype=torch.float32).T # Transpose to get shape (59, 3)\n",
    "\n",
    "print(f\"Human ratings tensor shape: {human_ratings_tensor.shape}\")\n",
    "print(f\"Human ratings tensor (first 5 rows):\\n{human_ratings_tensor[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) between scaled_sims and human_ratings: 1.8040\n",
      "\n",
      "Mean Absolute Percentage Error (MAPE): 39.74%\n",
      "\n",
      "Root Mean Squared Error (RMSE): 2.1785\n",
      "\n",
      "Pearson Correlation Coefficients (between scaled_sims and human_ratings):\n",
      "  Positive Dimension: 0.4015\n",
      "  Relaxed Dimension:  0.3448\n",
      "  Awake Dimension:    0.0842\n",
      "  Average Correlation: 0.2768\n",
      "\n",
      "R-squared scores:\n",
      "  valence = -2.706474781036377\n",
      "  tension = -2.3388662338256836\n",
      "  energy = -11.282599449157715\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr # For Pearson correlation\n",
    "\n",
    "# Comparison Method 1: Mean Absolute Error (MAE)\n",
    "# MAE measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "mae = torch.mean(torch.abs(scaled_sims - human_ratings_tensor))\n",
    "print(f\"Mean Absolute Error (MAE) between scaled_sims and human_ratings: {mae:.4f}\\n\")\n",
    "\n",
    "# Comparison Method 2: Mean Absolute Percentage Error (MAPE)\n",
    "# MAPE measures the accuracy of a forecasting method in terms of percentage.\n",
    "# Formula: MAPE = (1/n) * sum(|(Actual - Forecast) / Actual|) * 100%\n",
    "\n",
    "# Calculate the absolute percentage error for each element\n",
    "# Since human responses are between 1 and 9, division by zero is not a concern.\n",
    "absolute_percentage_error = torch.abs((human_ratings_tensor - scaled_sims) / human_ratings_tensor) * 100\n",
    "\n",
    "# Calculate the mean of these percentage errors\n",
    "mape = torch.mean(absolute_percentage_error)\n",
    "\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\\n\")\n",
    "\n",
    "# Comparison Method 3: Root Mean Squared Error (RMSE)\n",
    "# RMSE measures the square root of the average of the squared differences between predicted and actual values.\n",
    "# It gives a relatively high weight to large errors.\n",
    "# Formula: RMSE = sqrt(mean((Actual - Forecast)^2))\n",
    "\n",
    "# Calculate the squared differences\n",
    "squared_differences = (human_ratings_tensor - scaled_sims)**2\n",
    "\n",
    "# Calculate the mean of the squared differences (Mean Squared Error - MSE)\n",
    "mse = torch.mean(squared_differences)\n",
    "\n",
    "# Calculate the square root to get RMSE\n",
    "rmse = torch.sqrt(mse)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\")\n",
    "\n",
    "# Comparison Method 4: Pearson Correlation Coefficient (per column)\n",
    "# Pearson correlation measures the linear relationship between two sets of data.\n",
    "# We'll calculate it for each of the three columns (dimensions).\n",
    "\n",
    "correlation_positive, _ = pearsonr(scaled_sims[:, 0].detach().numpy(), human_ratings_tensor[:, 0].numpy())\n",
    "correlation_relaxed, _ = pearsonr(scaled_sims[:, 1].detach().numpy(), human_ratings_tensor[:, 1].numpy())\n",
    "correlation_awake, _ = pearsonr(scaled_sims[:, 2].detach().numpy(), human_ratings_tensor[:, 2].numpy())\n",
    "\n",
    "print(\"Pearson Correlation Coefficients (between scaled_sims and human_ratings):\")\n",
    "print(f\"  Positive Dimension: {correlation_positive:.4f}\")\n",
    "print(f\"  Relaxed Dimension:  {correlation_relaxed:.4f}\")\n",
    "print(f\"  Awake Dimension:    {correlation_awake:.4f}\")\n",
    "\n",
    "average_correlation = (correlation_positive + correlation_relaxed + correlation_awake) / 3\n",
    "print(f\"  Average Correlation: {average_correlation:.4f}\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# R-squared\n",
    "print(\"\\nR-squared scores:\")\n",
    "r2_valence = r2_score(human_ratings_tensor[:, 0].numpy(), scaled_sims[:, 0].detach().numpy())\n",
    "print(\"  valence =\", r2_valence)\n",
    "\n",
    "r2_tension = r2_score(human_ratings_tensor[:, 1].numpy(), scaled_sims[:, 1].detach().numpy())\n",
    "print(\"  tension =\", r2_tension)\n",
    "\n",
    "r2_energy = r2_score(human_ratings_tensor[:, 2].numpy(), scaled_sims[:, 2].detach().numpy())\n",
    "print(\"  energy =\", r2_energy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
