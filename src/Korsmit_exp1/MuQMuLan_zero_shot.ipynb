{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6b3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: change this to the path in your setup\n",
    "korsmit_exp1_path = \"../../data/Korsmit/Exp1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f635ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab94b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgios/Documents/Google DeepMind Research Ready Programme 2025/foundation_model_music_cognition/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/georgios/Documents/Google DeepMind Research Ready Programme 2025/foundation_model_music_cognition/.venv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from muq import MuQMuLan\n",
    "\n",
    "# This will automatically fetch checkpoints from huggingface\n",
    "device = 'cpu'\n",
    "mulan = MuQMuLan.from_pretrained(\"OpenMuQ/MuQ-MuLan-large\")\n",
    "mulan = mulan.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df0e32",
   "metadata": {},
   "source": [
    "# Process audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b960e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "stimuli_path = korsmit_exp1_path+\"Stimuli/\"\n",
    "\n",
    "for file in sorted(os.listdir(stimuli_path)):\n",
    "    if file.endswith(\".wav\"):\n",
    "        wav_path = os.path.join(stimuli_path, file)\n",
    "        audio, sample_rate = librosa.load(wav_path, sr=24000)\n",
    "        wav_tensor = torch.tensor(audio).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = mulan(wavs = wav_tensor)\n",
    "        embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7515679",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings = audio_embeddings.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a91c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([59, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1aa71",
   "metadata": {},
   "source": [
    "# Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f16d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I perceive this sound as happiness', 'I perceive this sound as sadness', 'I perceive this sound as anger', 'I perceive this sound as tenderness', 'I perceive this sound as fear']\n",
      "['This sound makes me feel happiness', 'This sound makes me feel sadness', 'This sound makes me feel anger', 'This sound makes me feel tenderness', 'This sound makes me feel fear']\n",
      "['I perceive this sound as positive', 'I perceive this sound as relaxed', 'I perceive this sound as awake']\n",
      "['This sound makes me feel positive', 'This sound makes me feel relaxed', 'This sound makes me feel awake']\n"
     ]
    }
   ],
   "source": [
    "discrete_tags = [\"happiness\", \"sadness\", \"anger\", \"tenderness\", \"fear\"]\n",
    "\n",
    "discrete_captions_perceived = [\"I perceive this sound as \" + tag for tag in discrete_tags]\n",
    "print(discrete_captions_perceived)\n",
    "discrete_captions_induced = [\"This sound makes me feel \" + tag for tag in discrete_tags]\n",
    "print(discrete_captions_induced)\n",
    "\n",
    "dimensional_tags = [\"positive\", \"relaxed\", \"awake\"]\n",
    "\n",
    "dimensional_captions_perceived = [\"I perceive this sound as \" + tag for tag in dimensional_tags]\n",
    "print(dimensional_captions_perceived)\n",
    "dimensional_captions_induced = [\"This sound makes me feel \" + tag for tag in dimensional_tags]\n",
    "print(dimensional_captions_induced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032319cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tag_embeds = mulan(texts = dimensional_captions_induced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0d14f",
   "metadata": {},
   "source": [
    "# Generate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 3])\n"
     ]
    }
   ],
   "source": [
    "sims = mulan.calc_similarity(audio_embeddings, tag_embeds)\n",
    "print(sims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "551f20b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5795e-02,  7.1918e-02,  1.2883e-01],\n",
       "        [-8.3636e-04,  6.9027e-02,  1.4914e-01],\n",
       "        [ 8.3426e-03,  3.5080e-02,  1.0751e-01],\n",
       "        [-1.9663e-02,  2.7963e-02,  1.1313e-01],\n",
       "        [ 4.7456e-02,  5.8541e-02,  1.6332e-01],\n",
       "        [ 6.2177e-02,  1.0858e-01,  2.0441e-01],\n",
       "        [-1.6177e-04,  3.9973e-02,  1.3816e-01],\n",
       "        [ 6.9783e-03,  1.0594e-01,  1.5483e-01],\n",
       "        [ 4.8948e-03,  1.2536e-01,  1.6362e-01],\n",
       "        [-2.9681e-02,  8.4724e-02,  1.1794e-01],\n",
       "        [-2.3657e-02,  4.3358e-02,  1.0542e-01],\n",
       "        [ 2.1669e-02,  7.7448e-02,  1.0986e-01],\n",
       "        [-6.0771e-02,  8.9668e-02,  8.9832e-02],\n",
       "        [ 6.2231e-02,  1.5983e-01,  1.8535e-01],\n",
       "        [-7.3319e-02,  4.9480e-02,  1.3519e-01],\n",
       "        [-7.3915e-02,  3.2677e-03,  1.1919e-01],\n",
       "        [-2.0476e-02,  1.2880e-02,  1.0707e-01],\n",
       "        [-4.2250e-02,  1.3218e-02,  9.9971e-02],\n",
       "        [-8.6838e-02,  1.0584e-02,  1.2069e-01],\n",
       "        [-1.5947e-02,  9.2721e-02,  1.6563e-01],\n",
       "        [-8.4143e-03,  8.9267e-02,  1.7489e-01],\n",
       "        [-1.1620e-02,  1.0926e-01,  1.7845e-01],\n",
       "        [ 2.6506e-02,  1.4653e-01,  1.7048e-01],\n",
       "        [-2.6643e-02,  4.8641e-02,  1.0954e-01],\n",
       "        [-4.4233e-02,  4.5283e-02,  1.2211e-01],\n",
       "        [-4.7687e-02, -9.6627e-03,  9.6945e-02],\n",
       "        [-4.0305e-02,  6.0373e-02,  1.4121e-01],\n",
       "        [-2.9529e-02,  6.5508e-02,  1.4182e-01],\n",
       "        [-1.5526e-02,  8.6501e-02,  1.6614e-01],\n",
       "        [ 3.3278e-02,  1.4553e-01,  1.6508e-01],\n",
       "        [-3.7434e-02,  6.1577e-02,  1.0980e-01],\n",
       "        [-1.0691e-01, -2.3497e-02,  2.9165e-02],\n",
       "        [ 1.9361e-03,  1.0462e-01,  1.3690e-01],\n",
       "        [ 7.9921e-02,  9.9244e-02,  1.5975e-01],\n",
       "        [ 8.6966e-02, -1.3084e-02,  6.6769e-02],\n",
       "        [-3.9318e-02, -4.9945e-03,  8.9126e-02],\n",
       "        [ 5.1813e-03, -7.2276e-02,  5.1622e-02],\n",
       "        [-2.7137e-02,  1.0083e-03,  8.7605e-02],\n",
       "        [-1.0277e-02,  9.9417e-02,  1.4879e-01],\n",
       "        [-1.5884e-02,  1.3975e-01,  1.5670e-01],\n",
       "        [ 2.4114e-02,  1.7895e-01,  1.8338e-01],\n",
       "        [-5.9383e-02,  2.6476e-02,  4.8123e-02],\n",
       "        [-7.2210e-02, -1.0008e-02,  4.8038e-02],\n",
       "        [-2.3552e-02,  8.2252e-02,  1.0440e-01],\n",
       "        [-5.7004e-02,  8.9339e-03,  5.8149e-02],\n",
       "        [-1.2225e-03,  4.7156e-02,  1.0807e-01],\n",
       "        [-3.2598e-03,  1.0270e-01,  1.5074e-01],\n",
       "        [-5.9582e-03,  9.1411e-04,  5.7901e-02],\n",
       "        [ 1.1996e-01,  1.4079e-01,  1.7209e-01],\n",
       "        [ 1.4861e-01,  9.4743e-02,  1.4456e-01],\n",
       "        [ 4.8186e-02,  8.8976e-03,  9.5700e-02],\n",
       "        [ 7.4149e-02, -2.1645e-02,  1.0155e-01],\n",
       "        [ 2.6307e-02,  1.0734e-01,  1.3477e-01],\n",
       "        [ 9.8301e-02,  1.7809e-01,  2.0178e-01],\n",
       "        [-1.3926e-02,  1.1991e-01,  1.1529e-01],\n",
       "        [ 9.0555e-02,  5.2768e-02,  1.4653e-01],\n",
       "        [-1.3638e-02,  1.1045e-01,  1.3918e-01],\n",
       "        [ 3.9440e-03,  1.5978e-01,  1.4401e-01],\n",
       "        [-2.1997e-02,  7.9549e-02,  1.4233e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a66c1",
   "metadata": {},
   "source": [
    "## Load csv files and extract related columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8682ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mean Vectors (Mean across CSVs for each row position) ---\n",
      "Length of positive_vector: 59\n",
      "Length of relaxed_vector: 59\n",
      "Length of awake_vector: 59\n"
     ]
    }
   ],
   "source": [
    "IDim_path = korsmit_exp1_path+\"Data/IDim/\"\n",
    "IDim_responses = []\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for file in os.listdir(IDim_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(IDim_path, file))\n",
    "        df.columns = df.columns.str.strip()\n",
    "        required_columns = ['positive', 'relaxed', 'awake']\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            all_dfs.append(df)\n",
    "\n",
    "mean_positive_vector = []\n",
    "mean_relaxed_vector = []\n",
    "mean_awake_vector = []\n",
    "\n",
    "num_rows = all_dfs[0].shape[0]\n",
    "\n",
    "for i in range(num_rows):\n",
    "    current_row_positives = []\n",
    "    current_row_relaxeds = []\n",
    "    current_row_awakes = []\n",
    "\n",
    "    # For the current row index 'i', collect values from all DataFrames\n",
    "    for df in all_dfs:\n",
    "        current_row_positives.append(df.iloc[i]['positive'])\n",
    "        current_row_relaxeds.append(df.iloc[i]['relaxed'])\n",
    "        current_row_awakes.append(df.iloc[i]['awake'])\n",
    "\n",
    "    # Calculate the mean for the current row across all files, for each column\n",
    "    mean_positive_vector.append(np.mean(current_row_positives))\n",
    "    mean_relaxed_vector.append(np.mean(current_row_relaxeds))\n",
    "    mean_awake_vector.append(np.mean(current_row_awakes))\n",
    "\n",
    "IDim_responses = {\n",
    "    'positive_vector': mean_positive_vector,\n",
    "    'relaxed_vector': mean_relaxed_vector,\n",
    "    'awake_vector': mean_awake_vector\n",
    "}\n",
    "\n",
    "print(\"\\n--- Mean Vectors (Mean across CSVs for each row position) ---\")\n",
    "print(f\"Length of positive_vector: {len(IDim_responses['positive_vector'])}\")\n",
    "print(f\"Length of relaxed_vector: {len(IDim_responses['relaxed_vector'])}\")\n",
    "print(f\"Length of awake_vector: {len(IDim_responses['awake_vector'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6f9d9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd51040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled sims shape: torch.Size([59, 3])\n",
      "Scaled sims (first 5 rows):\n",
      "tensor([[3.3413, 5.5953, 7.0578],\n",
      "        [3.7257, 5.5210, 7.5796],\n",
      "        [3.9616, 4.6487, 6.5100],\n",
      "        [3.2420, 4.4658, 6.6544],\n",
      "        [4.9667, 5.2516, 7.9440],\n",
      "        [5.3450, 6.5375, 9.0000],\n",
      "        [3.7431, 4.7744, 7.2974],\n",
      "        [3.9266, 6.4695, 7.7259],\n",
      "        [3.8730, 6.9686, 7.9516],\n",
      "        [2.9845, 5.9244, 6.7778]])\n",
      "\n",
      "Scaled sims min value: 1.0000\n",
      "Scaled sims max value: 9.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the min and max values in the current sims tensor\n",
    "old_min = sims.min()\n",
    "old_max = sims.max()\n",
    "\n",
    "# Define the new desired range\n",
    "new_min = 1.0\n",
    "new_max = 9.0\n",
    "\n",
    "# Apply the min-max scaling formula\n",
    "scaled_sims = ((sims - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(f\"Scaled sims shape: {scaled_sims.shape}\")\n",
    "print(f\"Scaled sims (first 5 rows):\\n{scaled_sims[:10]}\\n\")\n",
    "print(f\"Scaled sims min value: {scaled_sims.min():.4f}\")\n",
    "print(f\"Scaled sims max value: {scaled_sims.max():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3b9c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human ratings tensor shape: torch.Size([59, 3])\n",
      "Human ratings tensor (first 5 rows):\n",
      "tensor([[4.4872, 4.8798, 4.5185],\n",
      "        [4.7682, 5.3057, 4.7414],\n",
      "        [5.3105, 5.7312, 5.0760],\n",
      "        [5.2582, 5.2651, 5.4488],\n",
      "        [3.8897, 3.9540, 5.3549],\n",
      "        [4.3197, 3.8900, 5.5089],\n",
      "        [4.9825, 5.1875, 5.5354],\n",
      "        [5.0934, 4.8657, 5.8208],\n",
      "        [4.7605, 4.8502, 5.4158],\n",
      "        [5.2577, 4.8688, 5.7003]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "human_ratings_tensor = torch.tensor([\n",
    "    IDim_responses['positive_vector'],\n",
    "    IDim_responses['relaxed_vector'],\n",
    "    IDim_responses['awake_vector']\n",
    "], dtype=torch.float32).T # Transpose to get shape (59, 3)\n",
    "\n",
    "print(f\"Human ratings tensor shape: {human_ratings_tensor.shape}\")\n",
    "print(f\"Human ratings tensor (first 5 rows):\\n{human_ratings_tensor[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ca4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) between scaled_sims and human_ratings: 1.5578\n",
      "\n",
      "Mean Absolute Percentage Error (MAPE): 34.77%\n",
      "\n",
      "Root Mean Squared Error (RMSE): 1.9616\n",
      "\n",
      "Pearson Correlation Coefficients (between scaled_sims and human_ratings):\n",
      "  Positive Dimension: 0.2555\n",
      "  Relaxed Dimension:  -0.3114\n",
      "  Awake Dimension:    0.2678\n",
      "  Average Correlation: 0.0706\n",
      "\n",
      "R-squared scores:\n",
      "  valence = -2.250750780105591\n",
      "  tension = -2.301231861114502\n",
      "  energy = -5.558738708496094\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr # For Pearson correlation\n",
    "\n",
    "# Comparison Method 1: Mean Absolute Error (MAE)\n",
    "# MAE measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "mae = torch.mean(torch.abs(scaled_sims - human_ratings_tensor))\n",
    "print(f\"Mean Absolute Error (MAE) between scaled_sims and human_ratings: {mae:.4f}\\n\")\n",
    "\n",
    "# Comparison Method 2: Mean Absolute Percentage Error (MAPE)\n",
    "# MAPE measures the accuracy of a forecasting method in terms of percentage.\n",
    "# Formula: MAPE = (1/n) * sum(|(Actual - Forecast) / Actual|) * 100%\n",
    "\n",
    "# Calculate the absolute percentage error for each element\n",
    "# Since human responses are between 1 and 9, division by zero is not a concern.\n",
    "absolute_percentage_error = torch.abs((human_ratings_tensor - scaled_sims) / human_ratings_tensor) * 100\n",
    "\n",
    "# Calculate the mean of these percentage errors\n",
    "mape = torch.mean(absolute_percentage_error)\n",
    "\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\\n\")\n",
    "\n",
    "# Comparison Method 3: Root Mean Squared Error (RMSE)\n",
    "# RMSE measures the square root of the average of the squared differences between predicted and actual values.\n",
    "# It gives a relatively high weight to large errors.\n",
    "# Formula: RMSE = sqrt(mean((Actual - Forecast)^2))\n",
    "\n",
    "# Calculate the squared differences\n",
    "squared_differences = (human_ratings_tensor - scaled_sims)**2\n",
    "\n",
    "# Calculate the mean of the squared differences (Mean Squared Error - MSE)\n",
    "mse = torch.mean(squared_differences)\n",
    "\n",
    "# Calculate the square root to get RMSE\n",
    "rmse = torch.sqrt(mse)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\")\n",
    "\n",
    "# Comparison Method 4: Pearson Correlation Coefficient (per column)\n",
    "# Pearson correlation measures the linear relationship between two sets of data.\n",
    "# We'll calculate it for each of the three columns (dimensions).\n",
    "\n",
    "correlation_positive, _ = pearsonr(scaled_sims[:, 0].detach().numpy(), human_ratings_tensor[:, 0].numpy())\n",
    "correlation_relaxed, _ = pearsonr(scaled_sims[:, 1].detach().numpy(), human_ratings_tensor[:, 1].numpy())\n",
    "correlation_awake, _ = pearsonr(scaled_sims[:, 2].detach().numpy(), human_ratings_tensor[:, 2].numpy())\n",
    "\n",
    "print(\"Pearson Correlation Coefficients (between scaled_sims and human_ratings):\")\n",
    "print(f\"  Positive Dimension: {correlation_positive:.4f}\")\n",
    "print(f\"  Relaxed Dimension:  {correlation_relaxed:.4f}\")\n",
    "print(f\"  Awake Dimension:    {correlation_awake:.4f}\")\n",
    "\n",
    "average_correlation = (correlation_positive + correlation_relaxed + correlation_awake) / 3\n",
    "print(f\"  Average Correlation: {average_correlation:.4f}\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# R-squared\n",
    "print(\"\\nR-squared scores:\")\n",
    "r2_valence = r2_score(human_ratings_tensor[:, 0].numpy(), scaled_sims[:, 0].detach().numpy())\n",
    "print(\"  valence =\", r2_valence)\n",
    "\n",
    "r2_tension = r2_score(human_ratings_tensor[:, 1].numpy(), scaled_sims[:, 1].detach().numpy())\n",
    "print(\"  tension =\", r2_tension)\n",
    "\n",
    "r2_energy = r2_score(human_ratings_tensor[:, 2].numpy(), scaled_sims[:, 2].detach().numpy())\n",
    "print(\"  energy =\", r2_energy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
